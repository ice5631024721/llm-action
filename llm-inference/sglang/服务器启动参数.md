



- dtype：模型使用的数据类型，默认为 bfloat16。


["auto", "half", "float16", "bfloat16", "float", "float32"]

"Data type for model weights and activations.\n\n"
'* "auto" will use FP16 precision for FP32 and FP16 models, and '
"BF16 precision for BF16 models.\n"
'* "half" for FP16. Recommended for AWQ quantization.\n'
'* "float16" is the same as "half".\n'
'* "bfloat16" for a balance between precision and range.\n'
'* "float" is shorthand for FP32 precision.\n'
'* "float32" for FP32 precision.


- 参考：_get_and_verify_dtype


- kv_cache_dtype: kv 缓存的数据类型，默认为 dtype。

["auto", "fp8_e5m2", "fp8_e4m3"],

'Data type for kv cache storage. "auto" will use model data type. "fp8_e5m2" and "fp8_e4m3" is supported for CUDA 11.8+.',



fp8_e4m3 需要提供 quantization_param_path， 默认 scaling factors 为 1.0，会影响精度。


- schedule_policy

lpm
dfs-weight
fcfs - 默认
lof
random

```
model_path=/workspace/outputs/Qwen2.5-32B-Instruct-FP8-PER-TENSOR
dataset_path=/workspace/data/ShareGPT_V3_unfiltered_cleaned_split.json

CUDA_VISIBLE_DEVICES=6 python3 -m sglang.bench_offline_throughput \
        --model-path ${model_path} \
        --dataset-name random \
        --random-input 2048 \
        --random-output 1024 \
        --random-range-ratio 1 \
        --dataset-path  ${dataset_path} \
        --num-prompts 1000 \
        --mem-fraction-static 0.9 \
        --kv-cache-dtype fp8_e5m2 \
        --max-running-requests 48 \       
        --chunked-prefill-size 8192
        --schedule-policy lpm
```

--enable-torch-compile









