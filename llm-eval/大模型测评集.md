
https://github.com/open-compass/opencompass/blob/main/README_zh-CN.md


## 学科

MMLU


CMMLU---中文多任务语言理解评估

https://github.com/haonan-li/CMMLU


C-Eval

https://github.com/hkust-nlp/ceval


wget https://huggingface.co/datasets/ceval/ceval-exam/resolve/main/ceval-exam.zip



- https://zhuanlan.zhihu.com/p/629864222

从C-EVAL中选择了8个具有挑战性的数学、物理和化学科目，形成一个单独的基准， 即C-EVAL HARD，
包括高等数学、离散数学、概率和统计、大学化学、大学物理、高中数学、高中化学和高中物理。
这些科目通常涉及复杂的LATEX方程，需要非微妙的推理 能力来解决。



AGI-Eval



## 语言





## 知识


BoolQ


TriviaQA


NaturalQuestions



## 理解


CMRC


CSL


XSum


RACE (middle)


RACE (high)	


## 推理


WinoGrande

BBH


GSM8K


PIQA









## 编程


HumanEval




MBPP



## 大模型工具调用评测集


ToolBench 

## 长文本



https://github.com/THUDM/LongBench
https://modelscope.cn/datasets/ZhipuAI/LongBench/summary


```
from modelscope.msdatasets import MsDataset
ds = MsDataset.load('ZhipuAI/LongBench', subset_name='hotpotqa', split='test')


modelscope download --dataset ZhipuAI/LongBench

```



## LEval


https://github.com/OpenLMLab/LEval



## NeedleBench

- https://github.com/open-compass/opencompass

专门设计来压力测试LLMs在处理长达百万级上下文窗口中的检索能力和推理能力。



## LVEval


https://github.com/infinigence/LVEval/blob/main/README_ZH.md

LV-Eval: 5个长度等级、最长支持256k的长文本评测基准



LV-Eval中的所有数据都遵循以下格式。

```
{
    "input": "问题",
    "context": "上下文",
    "answers": "答案",
    "length": "上下文文本长度 (中文计字符数，英文计词数)",
    "dataset": "数据集名称",
    "language": "数据语言类型",
    "answer_keywords": "从答案中过滤得到的关键词",
    "confusing_facts": "插入到上下文中的干扰事实描述，增加测试难度",
}
```






















