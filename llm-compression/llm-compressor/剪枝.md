

https://docs.neuralmagic.com/guides/sparsification/


A tutorial doc for how to use the sparse prunings #993
- https://github.com/vllm-project/llm-compressor/issues/993


sparsegpt:
https://github.com/IST-DASLab/sparsegpt
https://arxiv.org/pdf/2301.00774

wanda:
https://github.com/locuslab/wanda
https://arxiv.org/pdf/2306.11695


SparseGPT、Wanda 和 MagnitudePruningModifier。

ConstantPruningModifier 仅用于在量化之前保持稀疏性。

```

ConstantPruningModifier:
    start: 0
    end: 5
    targets: __ALL_PRUNABLE__

MagnitudePruningModifier:
    start: 5
    end: 10
    init_sparsity: 0.1
    final_sparsity: 0.5
    targets: __ALL_PRUNABLE__


SparseGPTModifier:
    sparsity: 0.5
    mask_structure: "2:4"
    dampening_frac: 0.001
    block_size: 128
    targets: ['Linear']
    ignore: ['re:.*lm_head']



WandaPruningModifier:
    sparsity: 0.5
    mask_structure: "2:4"
    targets: __ALL_PRUNABLE__


```


oneshot() 为  compress() 应用别名，这是对 oneshot 的概括。

oneshot() 只运行 oneshot modifiers （GPTQ、SparseGPT 等），而 apply() 函数将运行配方中的所有阶段。

例如，如果您有一个配方，它指定了一个阶段用于微调，然后指定了一个阶段用于量化，则需要使用 apply()/compress() 来运行此配方，而不是 oneshot()


























